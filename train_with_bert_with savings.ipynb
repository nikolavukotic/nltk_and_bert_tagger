{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3mSZZJeiYWqn",
    "outputId": "34575803-8c4e-4092-e6de-98fe94e7b743"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting transformers\n",
      "  Downloading transformers-4.33.2-py3-none-any.whl (7.6 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.6/7.6 MB\u001b[0m \u001b[31m37.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.0.1+cu118)\n",
      "Requirement already satisfied: torchtext in /usr/local/lib/python3.10/dist-packages (0.15.2)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.12.2)\n",
      "Collecting huggingface-hub<1.0,>=0.15.1 (from transformers)\n",
      "  Downloading huggingface_hub-0.17.2-py3-none-any.whl (294 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m294.9/294.9 kB\u001b[0m \u001b[31m25.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.23.5)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2023.6.3)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n",
      "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1 (from transformers)\n",
      "  Downloading tokenizers-0.13.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m38.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting safetensors>=0.3.1 (from transformers)\n",
      "  Downloading safetensors-0.3.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m56.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.1)\n",
      "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch) (4.5.0)\n",
      "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.12)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.1)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.2)\n",
      "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch) (2.0.0)\n",
      "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch) (3.27.4.1)\n",
      "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch) (16.0.6)\n",
      "Requirement already satisfied: torchdata==0.6.1 in /usr/local/lib/python3.10/dist-packages (from torchtext) (0.6.1)\n",
      "Requirement already satisfied: urllib3>=1.25 in /usr/local/lib/python3.10/dist-packages (from torchdata==0.6.1->torchtext) (2.0.4)\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.15.1->transformers) (2023.6.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (2.1.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.2.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2023.7.22)\n",
      "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch) (1.3.0)\n",
      "Installing collected packages: tokenizers, safetensors, huggingface-hub, transformers\n",
      "Successfully installed huggingface-hub-0.17.2 safetensors-0.3.3 tokenizers-0.13.3 transformers-4.33.2\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers torch torchtext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "BgYGVGcfZI6F"
   },
   "outputs": [],
   "source": [
    "train_data_path = \"/content/srWaC_transformed_train.txt\"\n",
    "test_data_path = \"/content/srWaC_transformed_test.txt\"\n",
    "\n",
    "def load_data(data_path):\n",
    "    with open(data_path, \"r\", encoding=\"utf-8\") as file:\n",
    "        lines = file.readlines()\n",
    "    sentences = [line.strip().split() for line in lines]\n",
    "    return sentences\n",
    "\n",
    "train_sentences = load_data(train_data_path)\n",
    "test_sentences = load_data(test_data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Wxqj8YWNZRAB",
    "outputId": "831df9c2-15d7-4437-dc1b-1dd0eb4e64c6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Greška: Red ne ispunjava očekivanu strukturu: http://www.water-energy-food.org//X\n",
      "Greška: Red ne ispunjava očekivanu strukturu: //Z\n",
      "Greška: Red ne ispunjava očekivanu strukturu: //Z\n",
      "Greška: Red ne ispunjava očekivanu strukturu: http://www.ebay.com/itm/Western-Digital-My-Passport-Edge-Mac-500GB-/310650415709?_trksid=p2050601.m2372&amp;_trkparms=aid%3D111000%26algo%3DREC.CURRENT%26ao%3/X\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer\n",
    "\n",
    "# Koristićemo BERT model posebno treniran za srpski jezik\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "def tokenize_sentences(sentences):\n",
    "    tokenized = []\n",
    "    for sentence in sentences:\n",
    "        tokens = []\n",
    "        labels = []\n",
    "        for pair in sentence:\n",
    "            parts = pair.split(\"/\")\n",
    "            if len(parts) == 2:  # Očekujemo tačno dva dela\n",
    "                word, label = parts\n",
    "                tokens.extend(tokenizer.tokenize(word))\n",
    "                labels.extend([label] + [\"Prazno\"] * (len(tokenizer.tokenize(word)) - 1))\n",
    "            else:\n",
    "                # Ovde možete rukovati redovima koji ne ispunjavaju očekivanu strukturu\n",
    "                print(\"Greška: Red ne ispunjava očekivanu strukturu:\", pair)\n",
    "        tokenized.append((tokens, labels))\n",
    "    return tokenized\n",
    "\n",
    "train_tokenized = tokenize_sentences(train_sentences)\n",
    "test_tokenized = tokenize_sentences(test_sentences)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "io8UqNbYZVrN",
    "outputId": "0003a5f9-112c-4b76-aa75-665a9501863d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "400\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import torch\n",
    "\n",
    "labels_srWaC = ['AGA', 'AGD', 'AGG', 'AGI', 'AGL', 'AGN', 'AGV',\n",
    "\t'APA', 'APD', 'APG', 'API', 'APL', 'APN', 'APV',\n",
    "\t'ASA', 'ASD', 'ASG', 'ASI', 'ASL', 'ASN',\n",
    "\t'CC', 'CS',\n",
    "\t'I',\n",
    "\t'MC', 'MCA', 'MCD', 'MCG', 'MCI', 'MCL', 'MCN',\n",
    "\t'MM',\n",
    "\t'MO', 'MOA', 'MOD', 'MOG', 'MOI', 'MOL', 'MON',\n",
    "\t'MS', 'MSI', 'MSL', 'MSN',\n",
    "\t'NA', 'ND', 'NG', 'NI', 'NL', 'NN', 'NV',\n",
    "\t'PNA', 'PND', 'PNG', 'PNI', 'PNL', 'PNN',\n",
    "\t'PPA', 'PPD', 'PPG', 'PPI', 'PPL', 'PPN', 'PPV',\n",
    "\t'PXA', 'PXD', 'PXG', 'PXI', 'PXL',\n",
    "\t'QO', 'QQ', 'QR', 'QZ',\n",
    "\t'RG', 'RR', 'RS',\n",
    "\t'SA', 'SD', 'SG', 'SI', 'SL',\n",
    "\t'VAA', 'VAE', 'VAM', 'VAN', 'VAP', 'VAR',\n",
    "\t'VMA', 'VME', 'VMF', 'VMM', 'VMN', 'VMP', 'VMR',\n",
    "\t'X', 'XF', 'Y', 'Z', 'Prazno']\n",
    "\n",
    "max_token_length = max(len(tokens) for tokens, _ in train_tokenized + test_tokenized)\n",
    "\n",
    "print(max_token_length)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "OO691mbNaj-q"
   },
   "outputs": [],
   "source": [
    "def prepare_data(tokenized_data, tokenizer, max_length):\n",
    "    input_ids = []\n",
    "    attention_masks = []\n",
    "    labels = []\n",
    "\n",
    "    for tokens, label_ids in tokenized_data:\n",
    "        # Provera da li su tokens i label_ids prazni\n",
    "        if not tokens or not label_ids:\n",
    "            continue\n",
    "\n",
    "        # Mapiranje nepostojećih oznaka na \"Prazno\"\n",
    "        label_ids = [label if label in labels_srWaC else \"Prazno\" for label in label_ids]\n",
    "\n",
    "        encoded = tokenizer.encode_plus(tokens, padding=\"max_length\", max_length=max_length, truncation=True, add_special_tokens=True)\n",
    "        input_ids.append(encoded[\"input_ids\"])\n",
    "        attention_masks.append(encoded[\"attention_mask\"])\n",
    "\n",
    "        # Dodajte \"Prazno\" oznake na kraju liste label_ids kako biste dobili uniformnu dužinu\n",
    "        while len(label_ids) < max_length:\n",
    "            label_ids.append(\"Prazno\")\n",
    "\n",
    "        label_ids = [labels_srWaC.index(label) for label in label_ids]\n",
    "        labels.append(label_ids)\n",
    "\n",
    "    input_ids = torch.tensor(input_ids)\n",
    "    attention_masks = torch.tensor(attention_masks)\n",
    "    labels = torch.tensor(labels)\n",
    "\n",
    "    dataset = TensorDataset(input_ids, attention_masks, labels)\n",
    "    return dataset\n",
    "\n",
    "\n",
    "# Prvo pronađite maksimalnu dužinu tokena\n",
    "max_token_length = max(len(tokens) for tokens, _ in train_tokenized + test_tokenized)\n",
    "\n",
    "# Zatim koristite tu maksimalnu dužinu za tokenizaciju i kodiranje\n",
    "train_dataset = prepare_data(train_tokenized, tokenizer, max_length=max_token_length)\n",
    "test_dataset = prepare_data(test_tokenized, tokenizer, max_length=max_token_length)\n",
    "\n",
    "\n",
    "batch_size = 8\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kicDPlevbx4U",
    "outputId": "065a3716-a12d-47f0-f86e-af3116489167"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BertForTokenClassification(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=768, out_features=97, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import BertForTokenClassification, AdamW\n",
    "\n",
    "model = BertForTokenClassification.from_pretrained(\"bert-base-uncased\", num_labels=len(labels_srWaC))\n",
    "\n",
    "optimizer = AdamW(model.parameters(), lr=1e-5)\n",
    "\n",
    "# Postavite broj epoha i druge hiperparametre prema potrebi\n",
    "num_epochs = 5\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    for batch in train_dataloader:\n",
    "        input_ids, attention_mask, labels = batch\n",
    "        outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
    "        loss = outputs.loss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "# Evaluacija modela\n",
    "model.eval()\n",
    "# Dodajte kod za evaluaciju na test skupu podataka"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "EvonJ10m7lT8"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Putanja do direktorijuma u kojem želite da sačuvate model\n",
    "save_dir = r\"/content/\"\n",
    "\n",
    "# Kreirajte direktorijum ako ne postoji\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "# Sada možete sačuvati model\n",
    "model_save_path = os.path.join(save_dir, \"bert_custom_ner_model.pth\")\n",
    "torch.save(model.state_dict(), model_save_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "M1qnaEv78rhB",
    "outputId": "1cffc47e-fe0a-4f1a-bd8b-1bd8f8edb202"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:/Users/nikol/Desktop/MAS/SIR 2/bert/\n"
     ]
    }
   ],
   "source": [
    "print(save_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ulACFKf_80Ma",
    "outputId": "f6ec8290-adec-4220-8f9f-900caf9a09a0"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertForTokenClassification(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=768, out_features=97, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jA6nKlbmAFEE",
    "outputId": "0159c229-02c7-40ef-9a02-070474c840c8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tačnost modela na test skupu podataka: 95.39%\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "import numpy as np\n",
    "\n",
    "# Postavite model u režim evaluacije\n",
    "model.eval()\n",
    "\n",
    "# Inicijalizujte prazne liste za stvarne i predviđene oznake\n",
    "true_labels = []\n",
    "predicted_labels = []\n",
    "\n",
    "# Iterirajte kroz test dataloader\n",
    "for batch in test_dataloader:\n",
    "    input_ids, attention_mask, labels = batch\n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_ids, attention_mask=attention_mask)\n",
    "\n",
    "    # Izlazi modela su u formatu logita, koristite argmax da biste dobili predviđene oznake\n",
    "    predicted_batch_labels = torch.argmax(outputs.logits, dim=2).cpu().numpy()\n",
    "    true_batch_labels = labels.cpu().numpy()\n",
    "\n",
    "    # Dodajte oznake za ovaj batch u liste\n",
    "    predicted_labels.extend(predicted_batch_labels)\n",
    "    true_labels.extend(true_batch_labels)\n",
    "\n",
    "# Pretvorite listu u numpy niz za računanje tačnosti\n",
    "true_labels = np.concatenate(true_labels, axis=0)\n",
    "predicted_labels = np.concatenate(predicted_labels, axis=0)\n",
    "\n",
    "# Izračunajte tačnost\n",
    "accuracy = accuracy_score(true_labels, predicted_labels)\n",
    "\n",
    "print(f\"Tačnost modela na test skupu podataka: {accuracy * 100:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "z9y43Y_WFr8W",
    "outputId": "82986c74-8147-4176-cefa-517a27e55eff"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tačnost: 0.9539354838709677\n",
      "Preciznost: 0.9415873860120019\n",
      "Odziv: 0.9539354838709677\n",
      "F1-Skor: 0.9461468696240154\n",
      "Matrica konfuzije:\n",
      "[[     0      0      0 ...      0     58      5]\n",
      " [     0      0      0 ...      0      5      1]\n",
      " [     0      0      0 ...      0     90      8]\n",
      " ...\n",
      " [     0      0      0 ...      0     19      3]\n",
      " [     0      0      0 ...      0    747    100]\n",
      " [     0      0      0 ...      0     40 117222]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
    "\n",
    "# True labels su true_labels, a predicted labels su predicted_labels iz prethodnog odgovora\n",
    "\n",
    "# Tačnost\n",
    "accuracy = accuracy_score(true_labels, predicted_labels)\n",
    "\n",
    "# Preciznost\n",
    "precision = precision_score(true_labels, predicted_labels, average='weighted')\n",
    "\n",
    "# Odziv\n",
    "recall = recall_score(true_labels, predicted_labels, average='weighted')\n",
    "\n",
    "# F1-Skor\n",
    "f1 = f1_score(true_labels, predicted_labels, average='weighted')\n",
    "\n",
    "# Matrica konfuzije\n",
    "confusion = confusion_matrix(true_labels, predicted_labels)\n",
    "\n",
    "print(f\"Tačnost: {accuracy}\")\n",
    "print(f\"Preciznost: {precision}\")\n",
    "print(f\"Odziv: {recall}\")\n",
    "print(f\"F1-Skor: {f1}\")\n",
    "print(\"Matrica konfuzije:\")\n",
    "print(confusion)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "t_bkldIkNW01"
   },
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(self, gold, tagger_tags):\n",
    "  labels = labels_srWaC\n",
    "\tconfusion_matrix = skmetrics.confusion_matrix(gold, tagger_tags, labels=labels)\n",
    "\n",
    "\tdf_cm = pandas.DataFrame(confusion_matrix, index = [i for i in labels], columns = [i for i in labels])\n",
    "\tdf_cm.index.name = 'Actual'\n",
    "\tdf_cm.columns.name = 'Predicted'\n",
    "\tfig, ax = plt.subplots(figsize=(len(labels), len(labels)))\n",
    "\tseaborn.heatmap(df_cm, annot=True, fmt='', ax=ax)\n",
    "\tplt.savefig(self.model_name + \".png\")\n",
    "\tplt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "qQafyXdPRMgA"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Putanja do direktorijuma u kojem želite da sačuvate model\n",
    "save_dir = r\"/content/\"\n",
    "\n",
    "# Kreirajte direktorijum ako ne postoji\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "# Sada možete sačuvati model\n",
    "model_save_path = os.path.join(save_dir, \"bert_custom_ner_model.pth\")\n",
    "torch.save(model.state_dict(), model_save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "exX4KA2IX2Ba",
    "outputId": "5ebce22c-7694-49b1-9656-d3e905e0ebda"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertForTokenClassification(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=768, out_features=97, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#torch.save(model, '/content/bertsrWacfinetuned')\n",
    "sm = torch.load('/content/bertsrWacfinetuned')\n",
    "sm.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WOmLJwGIRRE8",
    "outputId": "e6a11d97-25bb-4468-aaa7-b2c4f09404ed"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model2 = BertForTokenClassification.from_pretrained(\"bert-base-uncased\", num_labels=len(labels_srWaC))\n",
    "model2.load_state_dict(torch.load(\"/content/bert_custom_ner_model.pth\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 418
    },
    "id": "c2-rNdTXUnb0",
    "outputId": "a8492902-c1aa-4ffa-8f51-7f0eb05d5738"
   },
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-23-aff78adefe79>\u001b[0m in \u001b[0;36m<cell line: 6>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mmodel2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'fine_tuned_bert'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mmodel2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTFBertForSequenceClassification\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/modeling_tf_utils.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m   2732\u001b[0m                     \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpretrained_model_name_or_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mWEIGHTS_INDEX_NAME\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2733\u001b[0m                 ):\n\u001b[0;32m-> 2734\u001b[0;31m                     raise EnvironmentError(\n\u001b[0m\u001b[1;32m   2735\u001b[0m                         \u001b[0;34mf\"Error no file named {TF2_WEIGHTS_NAME} found in directory {pretrained_model_name_or_path} \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2736\u001b[0m                         \u001b[0;34m\"but there is a file for PyTorch weights. Use `from_pt=True` to load this model from those \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mOSError\u001b[0m: Error no file named tf_model.h5 found in directory fine_tuned_bert but there is a file for PyTorch weights. Use `from_pt=True` to load this model from those weights."
     ]
    }
   ],
   "source": [
    "from transformers import TFBertForSequenceClassification\n",
    "\n",
    "model_name = 'fine_tuned_bert'\n",
    "\n",
    "model2.save_pretrained('fine_tuned_bert')\n",
    "model2 = TFBertForSequenceClassification.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QMv75_IkWh1H"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
